{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPjP8Zrx4yVh"
   },
   "source": [
    "# Masked image modeling with Autoencoders\n",
    "\n",
    "**Author:** [Lennart Seeger], [Aritra Roy Gosthipaty], [Sayak Paul]<br>\n",
    "**Date created:** 2021/12/20<br>\n",
    "**Last modified:** 2023/03/24<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.resnet import ResNet50\n",
    "import keras\n",
    "\n",
    "sys.path.insert(1, '../src')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data.datasets import get_mlrsnet, get_denmark\n",
    "from models.mae import prepare_data, get_test_augmentation_model, Patches, PatchEncoder, get_train_augmentation_model, get_test_augmentation_model, create_encoder, create_decoder, MaskedAutoencoder, mlp, TrainMonitor\n",
    "from model_utility.learning_rate_scheduler import WarmUpCosine\n",
    "from supportive.evaluate import evaluate_extractor\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acdAFEJU4yVp"
   },
   "outputs": [],
   "source": [
    "# data\n",
    "image_size = 64\n",
    "x_test, y_test = get_denmark(image_size=64)\n",
    "x_train=np.load(\"../data/avg_std30.npy\")\n",
    "\n",
    "# training\n",
    "buffer_size = 1024\n",
    "batch_size = 512\n",
    "auto = tf.data.AUTOTUNE\n",
    "num_classes = 25\n",
    "epochs = 50\n",
    "steps_per_epoch = len(x_train)//batch_size\n",
    "\n",
    "# OPTIMIZER\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "\n",
    "# patching and masking\n",
    "patch_size = 4\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "mask_proportion = 0.75\n",
    "input_shape = (image_size, image_size, 3)\n",
    "\n",
    "# encoder and decoder\n",
    "layer_norm_eps = 1e-6\n",
    "enc_projection_dim = 64\n",
    "dec_projection_dim = 32\n",
    "enc_num_heads = 2\n",
    "enc_layers = 2\n",
    "dec_num_heads = 1\n",
    "dec_layers = (\n",
    "    1  # The decoder is lightweight but should be reasonably deep for reconstruction.\n",
    ")\n",
    "enc_transformer_units = [\n",
    "    enc_projection_dim * 2,\n",
    "    enc_projection_dim,\n",
    "]  # Size of the transformer layers.\n",
    "dec_transformer_units = [\n",
    "    dec_projection_dim * 2,\n",
    "    dec_projection_dim,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pr2B8wl84yVq"
   },
   "outputs": [],
   "source": [
    "# manage the dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_ds = train_ds.shuffle(buffer_size).batch(batch_size).prefetch(auto)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "test_ds = test_ds.batch(batch_size).prefetch(auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pSclARO4yVr"
   },
   "outputs": [],
   "source": [
    "# Get a batch of images.\n",
    "image_batch = next(iter(train_ds))\n",
    "\n",
    "# Augment the images.\n",
    "augmentation_model = get_train_augmentation_model(input_shape, image_size)\n",
    "augmented_images = augmentation_model(image_batch)\n",
    "\n",
    "# Define the patch layer.\n",
    "patch_layer = Patches(patch_size)\n",
    "\n",
    "# Get the patches from the batched images.\n",
    "patches = patch_layer(images=augmented_images)\n",
    "\n",
    "# Now pass the images and the corresponding patches\n",
    "# to the `show_patched_image` method.\n",
    "random_index = patch_layer.show_patched_image(images=augmented_images, patches=patches)\n",
    "\n",
    "# Chose the same chose image and try reconstructing the patches\n",
    "# into the original image.\n",
    "image = patch_layer.reconstruct_from_patch(patches[random_index])\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFfPQdFc4yVs"
   },
   "outputs": [],
   "source": [
    "# Create the patch encoder layer.\n",
    "patch_encoder = PatchEncoder(patch_size, enc_projection_dim, mask_proportion)\n",
    "\n",
    "# Get the embeddings and positions.\n",
    "(\n",
    "    unmasked_embeddings,\n",
    "    masked_embeddings,\n",
    "    unmasked_positions,\n",
    "    mask_indices,\n",
    "    unmask_indices,\n",
    ") = patch_encoder(patches=patches)\n",
    "\n",
    "\n",
    "# Show a maksed patch image.\n",
    "new_patch, random_index = patch_encoder.generate_masked_image(patches, unmask_indices)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "img = patch_layer.reconstruct_from_patch(new_patch)\n",
    "plt.imshow(keras.utils.array_to_img(img))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Masked\")\n",
    "plt.subplot(1, 2, 2)\n",
    "img = augmented_images[random_index]\n",
    "plt.imshow(keras.utils.array_to_img(img))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qInV0QC_4yVu"
   },
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPNa2q3O4yVu"
   },
   "outputs": [],
   "source": [
    "train_augmentation_model = get_train_augmentation_model(input_shape, image_size)\n",
    "test_augmentation_model = get_test_augmentation_model(image_size)\n",
    "patch_layer = Patches(patch_size)\n",
    "patch_encoder = PatchEncoder(patch_size, enc_projection_dim, mask_proportion)\n",
    "encoder = create_encoder(enc_num_heads, enc_layers, enc_projection_dim, layer_norm_eps, enc_transformer_units)\n",
    "decoder = create_decoder(dec_layers, dec_num_heads, image_size, layer_norm_eps, dec_projection_dim, dec_transformer_units, num_patches, enc_projection_dim)\n",
    "\n",
    "mae_model = MaskedAutoencoder(\n",
    "    train_augmentation_model=train_augmentation_model,\n",
    "    test_augmentation_model=test_augmentation_model,\n",
    "    patch_layer=patch_layer,\n",
    "    patch_encoder=patch_encoder,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr-EiU204yVv"
   },
   "source": [
    "### Learning rate scheduler for Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuQlywA14yVv"
   },
   "outputs": [],
   "source": [
    "total_steps = steps_per_epoch * epochs\n",
    "warmup_epoch_percentage = 0.15\n",
    "warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
    "scheduled_lrs = WarmUpCosine(\n",
    "    learning_rate_base=learning_rate,\n",
    "    total_steps=total_steps,\n",
    "    warmup_learning_rate=0.0,\n",
    "    warmup_steps=warmup_steps,\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(8, 5),dpi=100)\n",
    "lrs = [scheduled_lrs(step) for step in range(total_steps)]\n",
    "plt.plot(lrs)\n",
    "plt.xlabel(\"Step\", fontsize=14)\n",
    "plt.ylabel(\"LR\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# save fig\n",
    "#fig.savefig(\"learning_rate_schedule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JqVch4N4yVw"
   },
   "source": [
    "## Model compilation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train callback\n",
    "train_callbacks = [TrainMonitor(epoch_interval=5, test_images=x_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tfa.optimizers.AdamW(learning_rate=scheduled_lrs, weight_decay=weight_decay)\n",
    "    \n",
    "# Compile and pretrain the model.\n",
    "mae_model.compile(\n",
    "    optimizer=optimizer, loss=keras.losses.MeanSquaredError(), metrics=[\"mae\"]\n",
    ")\n",
    "#mae_model.predict(x_test)\n",
    "history = mae_model.fit(\n",
    "    train_ds.repeat(), epochs=epochs, callbacks=[train_callbacks],steps_per_epoch = steps_per_epoch#val_ds#, validation_data=val_ds\n",
    ")\n",
    "\n",
    "# Measure its performance.\n",
    "loss, mae = mae_model.evaluate(test_ds)\n",
    "print(f\"Loss: {loss:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure performance\n",
    "\n",
    "loss, mae = mae_model.evaluate(test_ds)\n",
    "print(loss)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['mae'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss', 'mae'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8L-PFPFU4yVw"
   },
   "outputs": [],
   "source": [
    "# Extract components for extractor\n",
    "# Extract the augmentation layers.\n",
    "train_augmentation_model = mae_model.train_augmentation_model\n",
    "test_augmentation_model = mae_model.test_augmentation_model\n",
    "\n",
    "# Extract the patchers.\n",
    "patch_layer = mae_model.patch_layer\n",
    "patch_encoder = mae_model.patch_encoder\n",
    "patch_encoder.downstream = True\n",
    "\n",
    "# Extract the encoder.\n",
    "encoder = mae_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = prepare_data(image_size=image_size, images=x_test, labels=y_test, is_train=False, buffer_size=buffer_size, batch_size=batch_size, auto=auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "path=\"../model/mae/model\"\n",
    "encoder.save(path)\n",
    "model_loaded = keras.models.load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input((image_size, image_size, 3)),\n",
    "        get_test_augmentation_model(image_size),\n",
    "        patch_layer,\n",
    "        patch_encoder,\n",
    "        model_loaded,\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "    ],\n",
    "    name=\"extraction_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = ResNet50(weights='imagenet', include_top=False,input_shape=(image_size,image_size,3),pooling=\"avg\")\n",
    "print(\"extractor_model: \", evaluate_extractor(extractor_model.predict, x_test, y_test, neighbors=10))\n",
    "print(\"model_resnet: \", evaluate_extractor(model_resnet.predict, x_test, y_test, neighbors=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../model/mae/results.txt\", 'a') as file:\n",
    "        file.write('\\n')\n",
    "        file.write('\\n')\n",
    "        file.write('\\n')\n",
    "        file.write('------------------------------------------')\n",
    "        file.write(\"\\nlearning_rate: \"+str(learning_rate))\n",
    "        file.write(\"\\nbatch_size: \"+str(batch_size))\n",
    "        file.write(\"\\nepochs: \"+str(epochs))\n",
    "        file.write(\"\\npatch_size: \"+str(patch_size))\n",
    "        file.write(\"\\nmask_proportion: \"+str(mask_proportion))\n",
    "        file.write(\"\\nenc_projection_dim: \"+str(enc_projection_dim))\n",
    "        file.write(\"\\ndec_projection_dim: \"+str(dec_projection_dim))\n",
    "        file.write(\"\\noptimizer: \"+str(optimizer))\n",
    "        file.write(\"\\nenc_num_heads: \"+str(enc_num_heads))\n",
    "        file.write(\"\\nenc_layers: \"+str(enc_layers))\n",
    "        file.write(\"\\ndec_num_heads: \"+str(dec_num_heads))\n",
    "        file.write(\"\\ndec_layers: \"+str(dec_layers))\n",
    "        file.write('\\nneighbor_accuracy: '+str(evaluate_extractor(extractor_model.predict, x_test, y_test, neighbors=10)))\n",
    "        file.write('\\ntest_loss: '+str(loss))\n",
    "        file.write('\\ntest_mae: '+str(mae))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "masked_image_modeling",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
